{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modify the low-rank classification for extra unshared weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This modifies the 12-target (i12), low-rank classifier to use additional convolutional layers after classification. \n",
    "Before using this you should:\n",
    "- Have pycaffe setup and on the path\n",
    "- Have a copy of the low-rank solver.prototxt\n",
    "- Have a copy of the low-rank training classifier (e.g. `../scripts/anisotropic-training/training.prototxt`)\n",
    "- Have a copy of the low-rank inference classifier (e.g. `../scripts/anisotropic-training/deploy/deploy.prototxt`)\n",
    "\n",
    "Table of contents:\n",
    "- [Utility Functions](#Utility-Functions)\n",
    "- [Generate the Training Net](#Generate-the-net-to-use-for-training)\n",
    "- [Generate the Inference Net](#Generate-the-net-to-use-for-inference-/-testing)\n",
    "- [Test the Net](#Final-Test)\n",
    "- [Modify the Solver Prototxt](#Set-the-solver-parameters-to-use-the-new-net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google.protobuf.text_format\n",
    "import caffe\n",
    "caffe.set_mode_cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For debugging (cell can be removed from final notebook)\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_net_proto(path):\n",
    "    \"\"\"Read a net from a prototxt file\n",
    "    \n",
    "    :param path: The path to a caffe network (.prototxt file)\n",
    "    \n",
    "    :return: The prototxt object\n",
    "    :rtype: caffe.proto.caffe_pb2.NetParameter\n",
    "    \"\"\"\n",
    "    net = caffe.proto.caffe_pb2.NetParameter()\n",
    "    with open(path) as f:\n",
    "        proto = f.read()\n",
    "    google.protobuf.text_format.Parse(proto, net)\n",
    "    return net\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_net(train_net):\n",
    "    \"\"\" Print a (not sooo short) summary of the net layers\n",
    "    \n",
    "    :param net: A net \n",
    "    :type net: caffe.proto.caffe_pb2.NetParameter\n",
    "    \"\"\"\n",
    "    layers = list(train_net.layer)\n",
    "    for i, layer in enumerate(layers):\n",
    "        print \"{:04}\".format(i),\n",
    "        print \"\\t{:15}\\t{:15}\".format(layer.name, layer.type),\n",
    "        if layer.type==\"Convolution\":\n",
    "            if layer.convolution_param.kernel_size > 0:\n",
    "                print \"\\t{0:>2}x{0:<2}\".format(layer.convolution_param.kernel_size),\n",
    "            else:\n",
    "                print \"\\t{:>2}x{:<2}\".format(layer.convolution_param.kernel_w, layer.convolution_param.kernel_h),                \n",
    "        else:\n",
    "            print \"\\t{:5}\".format(''),\n",
    "\n",
    "        if \"_D\" in layer.name:\n",
    "            print \"DECODE\"\n",
    "        else:\n",
    "            print \"      \"\n",
    "\n",
    "    print \"Total\", len(layers), \"layers\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the net to use for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first time I tried to programatically modify a network, I wisely named it `modified-training-net.prototxt`. It is, in fact, a _low-rank-training-net.prototxt_ file and I wish I had named it that. I am too lazy to rename it and fix all references to it that would break; so there we are. \n",
    "\n",
    "This notebook will modify it again, but I will try to choose a more appropriate name this time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified-training-net.prototxt\r\n"
     ]
    }
   ],
   "source": [
    "!ls modified-training-net.prototxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_net = read_net_proto('modified-training-net.prototxt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, first non-shared-weights layers are 'conv-{LABEL}'.\n",
    "\n",
    "The probability scores are not in the net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = ['facade', 'window', 'door', 'cornice', 'sill', 'balcony', 'blind', 'deco', 'molding', 'pillar', 'shop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "facade 116\n",
      "window 117\n",
      "door 118\n",
      "cornice 119\n",
      "sill 120\n",
      "balcony 121\n",
      "blind 122\n",
      "deco 123\n",
      "molding 124\n",
      "pillar 125\n",
      "shop 126\n"
     ]
    }
   ],
   "source": [
    "num_layers = len(train_net.layers)\n",
    "\n",
    "indices = {}\n",
    "for i,l in enumerate(train_net.layer):\n",
    "    indices[l.name] = i\n",
    "    \n",
    "for label in LABELS:\n",
    "    print label, indices['conv-'+label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<caffe_pb2.LayerParameter at 0x7ff07999b050>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top off the shared weights with a (full rank) convolution that has the\n",
    "# same number of outputs that our concatenation layer will have\n",
    "\n",
    "def make_initial_layer():\n",
    "    layer = caffe.proto.caffe_pb2.LayerParameter()\n",
    "    layer.type =\"Convolution\"\n",
    "    layer.name = \"conv1_1_D\"\n",
    "    layer.bottom.append('conv1_2_D')\n",
    "    layer.top.append('conv1_1_D')\n",
    "\n",
    "    layer.convolution_param.num_output = 4*len(LABELS)\n",
    "    layer.convolution_param.pad.append(1)\n",
    "    layer.convolution_param.kernel_size.append(3)\n",
    "    layer.convolution_param.weight_filler.type='msra'\n",
    "    layer.convolution_param.bias_filler.type='constant'\n",
    "\n",
    "    layer.param.add(lr_mult=1, decay_mult=1)\n",
    "    layer.param.add(lr_mult=1, decay_mult=0)\n",
    "\n",
    "    return layer\n",
    "\n",
    "make_initial_layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<caffe_pb2.LayerParameter at 0x7ff07999b5f0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate\n",
    "concat = caffe.proto.caffe_pb2.LayerParameter()\n",
    "concat.type =\"Concat\"\n",
    "concat.name = \"concat\"\n",
    "concat.top.append('concat')\n",
    "for label in LABELS:\n",
    "    concat.bottom.append('conv-' + label)\n",
    "concat.concat_param.axis = 1\n",
    "\n",
    "concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create recurrent layers; \n",
    "\n",
    "# QUESTION: To the _output_ blocks require different names?\n",
    "# Or will the network be able to understand this structure?\n",
    "\n",
    "def make_recurrant_feature_node(target_name, pass_index):\n",
    "    rec_xxx = caffe.proto.caffe_pb2.LayerParameter()\n",
    "    rec_xxx.CopyFrom(train_net.layer[indices['conv-'+target_name]])\n",
    "    rec_xxx.name = 'conv_{}_r{}'.format(target_name, pass_index)\n",
    "    if pass_index == 1:\n",
    "        rec_xxx.bottom[0] = 'conv1_1_D'\n",
    "    else:\n",
    "        rec_xxx.bottom[0] = 'concat_r{}'.format(pass_index-1)\n",
    "    rec_xxx.top[0] = 'conv_{}_r{}'.format(target_name, pass_index)\n",
    "    rec_xxx.param[0].name='conv_{}_W'.format(target_name)\n",
    "    rec_xxx.param[1].name='conv_{}_b'.format(target_name)\n",
    "    return rec_xxx\n",
    "    \n",
    "def make_reccurant_concat_node(pass_index):\n",
    "    concat = caffe.proto.caffe_pb2.LayerParameter()\n",
    "    concat.type =\"Concat\"\n",
    "    concat.name = 'concat_r{}'.format(pass_index)\n",
    "    concat.top.append('concat_r{}'.format(pass_index))\n",
    "    for label in LABELS:\n",
    "        concat.bottom.append('conv_{}_r{}'.format(label, pass_index))\n",
    "    concat.concat_param.axis = 1\n",
    "    return concat\n",
    " \n",
    "def make_recurrant_block(pass_index):\n",
    "    block = []\n",
    "    for target_name in LABELS:\n",
    "        block.append(make_recurrant_feature_node(target_name, pass_index))\n",
    "    block.append(make_reccurant_concat_node(pass_index))\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Set bottoms for conv-xxx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And insert them after the existing 'conv' layers ('conv-shop')\n",
    "def stitch_in_new_layers(net):\n",
    "    indices = {l.name:i for i, l in enumerate(net.layer)}\n",
    "    conv1_1_D_index = indices['conv-'+LABELS[0]]\n",
    "    reccurant_start_index = indices['conv-'+LABELS[-1]]+1\n",
    "\n",
    "    layers = list(net.layer)\n",
    "\n",
    "    prequel = layers[:conv1_1_D_index]\n",
    "    target_layers = layers[conv1_1_D_index:reccurant_start_index]\n",
    "    sequel = layers[reccurant_start_index:]\n",
    "\n",
    "    rec_blocks = []\n",
    "    for pass_index in range(1, 5+1):\n",
    "        rec_blocks += make_recurrant_block(pass_index)\n",
    "\n",
    "    del net.layer[:] \n",
    "    net.layer.extend(prequel)\n",
    "    net.layer.extend([make_initial_layer()])\n",
    "    net.layer.extend(rec_blocks)\n",
    "    \n",
    "    for layer in target_layers:\n",
    "        layer.bottom[0] = 'concat_r5'\n",
    "        target_name = layer.name.replace('conv-','')\n",
    "        layer.name = layer.name + '_final'\n",
    "        layer.param[0].name='conv_{}_W'.format(target_name)\n",
    "        layer.param[1].name='conv_{}_b'.format(target_name)\n",
    "    \n",
    "    net.layer.extend(target_layers)\n",
    "    net.layer.extend(sequel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_net_proto(path, net):\n",
    "    new_proto = google.protobuf.text_format.MessageToString(net)\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(new_proto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stitch_in_new_layers(train_net)\n",
    "save_net_proto('compatability-training-net.prototxt', train_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the net to use for inference / testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before executing these cells, copy the net used to do inference with the 'low-rank' classifier into this folder and name it 'modified-inference-net.prototxt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_net_path = 'modified-inference-net.prototxt'\n",
    "infer_net = read_net_proto(infer_net_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stitch_in_new_layers(infer_net)\n",
    "save_net_proto('compatability-inference-net.prototxt', infer_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save a non-bayesian version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolution,ArgMax,BN,Upsample,ReLU,Pooling,Softmax,Dropout,Concat\n"
     ]
    }
   ],
   "source": [
    "print ','.join(set([layer.type for layer in infer_net.layer]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 dropout layers\n"
     ]
    }
   ],
   "source": [
    "dropouts = [layer for layer in infer_net.layer if layer.type == 'Dropout']\n",
    "print \"Found\", len(dropouts), \"dropout layers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropout encdrop3 _was_ set to work during testing... not anymore though!\n",
      "Dropout encdrop4 _was_ set to work during testing... not anymore though!\n",
      "Dropout encdrop5 _was_ set to work during testing... not anymore though!\n",
      "Dropout decdrop5 _was_ set to work during testing... not anymore though!\n",
      "Dropout decdrop4 _was_ set to work during testing... not anymore though!\n",
      "Dropout decdrop3 _was_ set to work during testing... not anymore though!\n"
     ]
    }
   ],
   "source": [
    "for dropout in dropouts:\n",
    "    if dropout.dropout_param.HasField('sample_weights_test'):\n",
    "        print \"Dropout\", dropout.name, \"_was_ set to work during testing...\",\n",
    "        dropout.dropout_param.ClearField('sample_weights_test')\n",
    "        print \"not anymore though!\"\n",
    "    else:\n",
    "        print \"Dropout\", dropout.name, \"is NOT set to work during testing...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_net_proto('non-bayesian-compatability-inference-net.prototxt', infer_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Test \n",
    "** Make Sure Things Load Right**\n",
    "- The net should load without crashing everything\n",
    "- The tops should ONLY be the 'label-' layers, or the 'loss' layers, if everything is connected properly"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install pydot\n",
    "!apt-get install -y graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drawing net to compatability-training-net.svg\n"
     ]
    }
   ],
   "source": [
    "%run /opt/caffe-segnet-cudnn5/python/draw_net.py 'compatability-training-net.prototxt'  'compatability-training-net.svg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[output figure](compatability-training-net.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/low-rank/pyfacades/models/independant_12_layers/__init__.py:2: UserWarning: \n",
      "This call to matplotlib.use() has no effect because the backend has already\n",
      "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "The backend was *originally* set to u'nbAgg' by the following code:\n",
      "  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n",
      "    \"__main__\", fname, loader, pkg_name)\n",
      "  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n",
      "    exec code in run_globals\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 486, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py\", line 888, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2828, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-c22e755e8d38>\", line 1, in <module>\n",
      "    get_ipython().magic(u'pylab notebook')\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n",
      "    return self.run_line_magic(magic_name, magic_arg_s)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n",
      "    result = fn(*args,**kwargs)\n",
      "  File \"<decorator-gen-106>\", line 2, in pylab\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n",
      "    call = lambda f, *a, **k: f(*a, **k)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/magics/pylab.py\", line 156, in pylab\n",
      "    gui, backend, clobbered = self.shell.enable_pylab(args.gui, import_all=import_all)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2989, in enable_pylab\n",
      "    gui, backend = self.enable_matplotlib(gui)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n",
      "    pt.activate_matplotlib(backend)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n",
      "    matplotlib.pyplot.switch_backend(backend)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n",
      "    matplotlib.use(newbackend, warn=False, force=True)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/matplotlib/__init__.py\", line 1305, in use\n",
      "    reload(sys.modules['matplotlib.backends'])\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n",
      "    line for line in traceback.format_stack()\n",
      "\n",
      "\n",
      "  matplotlib.use('Agg')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized 'TrainInputLayer\n",
      "   For fold number 1\n",
      "   Getting files from /root/low-rank/data/training/independant_12_layers/fold_01/train.txt\n",
      "   Found 1173 files.\n",
      "source: /root/low-rank/data/training/independant_12_layers/fold_01/train.txt\n",
      "number of files: 1173\n",
      "/root/low-rank/data/training/independant_12_layers/cmp/npy/stack_0651.npy\n"
     ]
    }
   ],
   "source": [
    "import caffe\n",
    "caffe.set_device(0)\n",
    "caffe.set_mode_gpu()\n",
    "\n",
    "import facade_layers\n",
    "reload(facade_layers)\n",
    "net = caffe.Net('compatability-training-net.prototxt', \n",
    "                'test_weights_from_peihao.caffemodel', caffe.TRAIN)\n",
    "\n",
    "results = net.forward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I have properly connected things, these should be just the 'loss' layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['shop-loss',\n",
       " 'blind-loss',\n",
       " 'cornice-loss',\n",
       " 'door-loss',\n",
       " 'sill-loss',\n",
       " 'deco-loss',\n",
       " 'facade-loss',\n",
       " 'window-loss',\n",
       " 'molding-loss',\n",
       " 'balcony-loss',\n",
       " 'pillar-loss']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "del net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the solver parameters to use the new net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.protobuf.text_format import Parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<caffe_pb2.SolverParameter at 0x7ff05097ae60>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solver = caffe.proto.caffe_pb2.SolverParameter()\n",
    "Parse(open('solver.prototxt').read(), solver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver.net = os.path.abspath('compatability-training-net.prototxt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls /data/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver.snapshot_prefix='/data/models/compatability_net'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_iter: 1\n",
      "test_interval: 10000000\n",
      "base_lr: 1e-05\n",
      "display: 10\n",
      "max_iter: 250000\n",
      "lr_policy: \"step\"\n",
      "gamma: 1.0\n",
      "momentum: 0.95\n",
      "weight_decay: 0.0005\n",
      "stepsize: 10000000\n",
      "snapshot: 1000\n",
      "snapshot_prefix: \"/data/models/compatability_net\"\n",
      "solver_mode: GPU\n",
      "net: \"/root/low-rank/scripts/anisotropic-training/compatability-training-net.prototxt\"\n",
      "test_initialization: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('compatability-solver.prototxt', 'w') as f:\n",
    "    f.write(google.protobuf.text_format.MessageToString(solver))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we can start training **almost**. This cost me 1000 iterations, I forgot to make the folder that will hold the snapshots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.isdir(os.path.dirname(solver.snapshot_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok, it's alright, we must have already made that directory.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.makedirs(os.path.dirname(solver.snapshot_prefix))\n",
    "    print \"Since caffe won't do it for us (ugh!) I made the folder needed to hold our snapshots\"\n",
    "except OSError as e:\n",
    "    print \"Ok, it's alright, we must have already made that directory.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting compatability-start-training.sh\n"
     ]
    }
   ],
   "source": [
    "%%file compatability-start-training.sh\n",
    "#!/usr/bin/env bash\n",
    "\n",
    "INITIAL_WEIGHTS=${PWD}/test_weights_from_peihao.caffemodel\n",
    "\n",
    "caffe.bin train -solver=compatability-solver.prototxt -weights ${INITIAL_WEIGHTS} -gpu=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing compatability-resume-training\n"
     ]
    }
   ],
   "source": [
    "%%file compatability-resume-training\n",
    "#!/usr/bin/env bash\n",
    "caffe.bin train -solver=compatability-solver.prototxt \\\n",
    "                -snapshot $(./get_iter.py -s -p compatability-solver.prototxt) \\\n",
    "                -gpu=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I should be able to run 'compatability-start-training.sh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
