{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAQ / Common Issues\n",
    "\n",
    "This was written on the original caffe SEGNET, with a Tesla K40 GPU. Caffe was compiled with cudnn version 3. This version of cudnn is not compatible with some newer GPUs.\n",
    "\n",
    "* If you get bad results (poor accuracy, garbage, etc), your GPU may be too old. Try using [https://hub.docker.com/r/jfemiani/caffe_segnet_cudnn5/](jfemiani/caffe_segnet_cudnn5) which is a docker image with SEGNET on cudnn 5. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch the evaluation / holdout data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_txt = '/data/traintest/i12/fold_01/eval.txt'\n",
    "eval_files = [fn.strip() for fn in open(eval_txt).readlines()]\n",
    "print \"Found {} eval files\".format(len(eval_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'new' plan is to map a global `/data` to the location of the data files. This can be one easily with a docker volume command, which is how we do it. It can also be accomplished via a soft link on linux. This is better than assuming the data is in the current folder, since it is too big to store multiple times or commit to git, and symlinks do not commit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_files = [fn.replace('data/training/independant_12_layers', '/data/traintest/i12') for fn in eval_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp_eval_files = [f for f in eval_files if '/cmp/' in f]\n",
    "print \"Found\", len(cmp_eval_files), \"from the CMP dataset\", len(eval_files)-len(cmp_eval_files), \"are our own\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images, nrows=4, ncols=4):\n",
    "    for i in range(nrows*ncols):\n",
    "        subplot(nrows, ncols, i+1)\n",
    "        labeled_data = np.load(images[i])\n",
    "        rgb = labeled_data[:3]\n",
    "        features = labeled_data[3:]\n",
    "        window  = features[1]\n",
    "        imshow(rgb.transpose(1,2,0)/255.)\n",
    "        xticks([]); yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(cmp_eval_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyfacades.util.metrics import Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyfacades.util.metrics\n",
    "reload(pyfacades.util.metrics)\n",
    "Metrics = pyfacades.util.metrics.Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS= ['background', 'facade', 'window', 'door', 'cornice', 'sill',\n",
    "         'balcony', 'blind', 'deco', 'molding', 'pillar', 'shop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyfacades.util.metrics!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before executing this next cell make sure you have acess to one of the GPU's. You will want one that nobody else is using. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import caffe\n",
    "#caffe.set_mode_cpu()\n",
    "caffe.set_mode_gpu()\n",
    "caffe.set_device(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason, it seems like the terminal pauses partway through loading the moel. I have no idea why, but it can be resolved by switching over to the terminal that is running the notebook and hitting enter.\n",
    "\n",
    "In general, whenever you run a caffe operation it is a good idea to pek at the terminal to see what it is spitting out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROTO = 'non-bayesian-inference-net.prototxt'\n",
    "#WEIGHTS = 'deploy/test_weights.caffemodel'\n",
    "WEIGHTS = 'test_weights_from_peihao.caffemodel'\n",
    "net = caffe.Net(PROTO, WEIGHTS, caffe.TEST)\n",
    "\n",
    "# Set the batch size to one -- we are using the CPU\n",
    "net.blobs['data'].reshape(1, 3, 512, 512)\n",
    "net.reshape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each target in LABELS, we assign a label from this set (NEG, UNK, POS, EDGE).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEG = 0\n",
    "UNK = 1\n",
    "POS = 2\n",
    "EDG = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1 score will depend on the amount of dilation. \n",
    "Right now the dilation is hardcoded as 3.\n",
    "(This is done in the `Metrics` class)\n",
    "We _should_ search for best value..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a, axis=0):\n",
    "    a = np.exp(a - a.max(axis=axis))\n",
    "    a /= a.sum(axis=axis)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_NAME = 'window'\n",
    "BACKGROUND=LABELS.index('background')\n",
    "\n",
    "f = cmp_eval_files[9]\n",
    "\n",
    "def get_metrics(f, target_name=TARGET_NAME, with_edges=True):\n",
    "    blob = np.load(f)\n",
    "    rgb = blob[:3]\n",
    "    expected = blob[3:]  \n",
    "    target_index = LABELS.index(target_name)\n",
    "    expected_target = expected[target_index]\n",
    "    expected_target[expected[BACKGROUND]==2] =1 # ignore\n",
    "    \n",
    "    # Remove padding from considereation\n",
    "    # (Padding was added to work around a bug in caffe; every image needed at least one pixel that is not ignored.)\n",
    "    padding = rgb.max(axis=0).max(axis=0) == 0\n",
    "    padding = np.outer(np.ones(rgb.shape[1]), padding) > 0\n",
    "    expected_target[padding] = 1\n",
    "  \n",
    "    net.forward(data=np.array([rgb]))\n",
    "    predicted_probs = net.blobs['prob-'+target_name].data[0].squeeze()\n",
    "   \n",
    "    if with_edges is False:\n",
    "        predicted_probs[(NEG, POS),:,:] = softmax(predicted_probs[(NEG, POS),:,:].copy())\n",
    "        predicted_probs[EDG] = 0\n",
    "    elif with_edges == 'pos':\n",
    "        predicted_probs[POS,:,:] += predicted_probs[EDG,:,:]\n",
    "        predicted_probs[EDG] = 0\n",
    "        predicted_probs[(NEG, POS),:,:] = softmax(predicted_probs[(NEG, POS),:,:].copy())\n",
    "        \n",
    "    predicted_target = predicted_probs.argmax(0)\n",
    "\n",
    "    #set_trace()\n",
    "    mf = Metrics(expected=expected_target, \n",
    "                 predicted=predicted_target, \n",
    "                 label_positive=POS,\n",
    "                 label_negative=NEG,\n",
    "                 source=f,\n",
    "                 feature=target_name\n",
    "                )\n",
    "    return mf, rgb, expected_target, predicted_target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_eval(rgb, expected, predicted):       \n",
    "    colors = array([[255, 0,   0],\n",
    "                    [128, 128, 128], \n",
    "                    [0,   255, 0], \n",
    "                    [128, 128, 128]], dtype=np.uint8)\n",
    "    subplot(131)\n",
    "    imshow(rgb.transpose(1,2,0)/255.)\n",
    "    axis('off')\n",
    "    subplot(132)\n",
    "    imshow(rgb.transpose(1,2,0)/255.)\n",
    "    imshow(colors[expected.astype(int)], alpha=0.5)\n",
    "    axis('off')\n",
    "    subplot(133)\n",
    "    imshow(rgb.transpose(1,2,0)/255.)\n",
    "    imshow(colors[predicted.astype(int)], alpha=0.5)\n",
    "    axis('off')predicted_probs[(NEG, POS),:,:] = softmax(predicted_probs[(NEG, POS),:,:].copy())\n",
    "    tight_layout()\n",
    "    \n",
    "fig = figure(figsize=(12, 4))\n",
    "visualize_eval(rgb, expected, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I do not _necessarily_ want to re-run the net for images I have already processed. I am using anydbm to check if the image has already been evaluated.\n",
    "\n",
    "The way this works is this:\n",
    "1. I create a database (key->value) and I associate each filename (the key) with the evaluation metrics for that file. The purpose of using anydb vs a python `dict` is that I do not want to lose the cache just because the server restarts. \n",
    "2. The metrics are calculated for all files in the evaluation set, if not already in the db, and their summary is displayed. \n",
    "3. I use the already-computed metrics to print a summary for _just_ the CMP images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anydbm\n",
    "import json\n",
    "import hashlib\n",
    "import munch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(target_name, eval_files, recompute=True, visualize=False, cache=None, with_edges=True):\n",
    "    if cache is None:\n",
    "        cache = {}\n",
    "    elif recompute == True:\n",
    "        cache.clear()\n",
    "\n",
    "    if visualize:\n",
    "        fig = figure(figsize=(12,4))\n",
    "\n",
    "    total = Metrics(feature=target_name)\n",
    "    for i, f in enumerate(eval_files):   \n",
    "        if f not in cache:\n",
    "            mf, rgb, expected, predicted = get_metrics(f, target_name=target_name, with_edges=with_edges)\n",
    "            cache[f] = json.dumps(mf.as_dict())\n",
    "            if visualize:\n",
    "                visualize_eval(rgb, expected, predicted)\n",
    "                try:\n",
    "                    suptitle('{} of {}, $P$:{:.2%}, $R$:{:.2%}, $F_1$:{:.2%}, $A$:{:.2%}'.format(i, len(eval_files), total.pixel_precision, total.pixel_recall, total.pixel_f1, total.pixel_accuracy))\n",
    "                except ZeroDivisionError:\n",
    "                    suptitle(\"Not enough samples yet....\")\n",
    "\n",
    "                fig.canvas.draw()\n",
    "        else:\n",
    "            mf = Metrics(**json.loads(cache[f]))\n",
    "        #print mf, \n",
    "        total += mf\n",
    "        print '\\r{:3} of {}'.format(i, len(eval_files)),\n",
    "    return total"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "predicted_probs[(NEG, POS),:,:] = softmax(predicted_probs[(NEG, POS),:,:].copy())cmp_total = compute_metrics(target_name='window', \n",
    "                            eval_files=cmp_eval_files,  \n",
    "                            recompute=False,\n",
    "                            cache=anydbm.open('eval_low_rank_'+TARGET_NAME, 'c'))\n",
    "\n",
    "\n",
    "print cmp_total, 'Acc:', cmp_total.pixel_accuracy\n",
    "\n",
    "from IPython.display import Latex\n",
    "latex = r\"\"\"\n",
    "\\begin{{array}}{{ccccccc}}\n",
    "Approach  & Acc & P & R & F_1 & P_{{obj}} & R_{{obj}} & F_{{1obj}}  \\\\\n",
    "\\hline\n",
    "Separable & {acc:.2f} & {pixel_precision:.2f} & {pixel_recall:.2f} & {pixel_f1:.2f} & {object_precision:.2f} & {object_recall:.2f} & {object_f1:.2f}\\\\\n",
    "\\end{{array}}\n",
    "\"\"\".format(acc=cmp_total.pixel_accuracy, **cmp_total.as_dict())\n",
    "\n",
    "print latex\n",
    "Latex(latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totals = {}\n",
    "caches = {}\n",
    "for target_name in LABELS[2:]:\n",
    "    print \"Evaluating\", target_name\n",
    "    caches[target_name] = {}\n",
    "    totals[target_name] = compute_metrics(target_name=target_name, eval_files=cmp_eval_files, \n",
    "                                          with_edges='pos',  # If false, exclude 'conv-edges' from softmax\n",
    "                                          cache=caches[target_name])\n",
    "\n",
    "    print totals[target_name], 'Acc:',  totals[target_name].pixel_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Latex\n",
    "latex = r\"\"\"\n",
    "\\begin{array}{rcccccc}\n",
    "Target  & Acc & P & R & F_1 & P_{ob} & R_{ob} & F_{1ob}  \\\\\n",
    "\\hline\n",
    "\"\"\"\n",
    "for label in sorted(totals.keys()):\n",
    "        row = r\"\\texttt{{ {target} }} & {acc:.2f} & {pixel_precision:.2f} & {pixel_recall:.2f} & {pixel_f1:.2f} & {object_precision:.2f} & {object_recall:.2f} & {object_f1:.2f}\\\\\" + \"\\n\"\n",
    "        row = row.format(target=label, acc=totals[label].pixel_accuracy, **totals[label].as_dict())\n",
    "        latex += row\n",
    "latex += \"\"\"\\end{array}\n",
    "\"\"\"\n",
    "\n",
    "print latex\n",
    "Latex(latex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an earlier version of this code, I did not keep track of the accuracy but only the precision (P) and recall (R). If we know the number of samples, it is possible to reverse-engineer an accuracy so that we can compare against others who report only accuaracy. \n",
    "\n",
    "I am confused about why people report accuracy for this imbalanced problem, and even skeptical that their numebrs are infact accuracy and not e.g. precision, but this is what we have to work with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_engineer_acc(P, R):\n",
    "    pos = 5988721\n",
    "    neg = 57149895\n",
    "    TP = R*pos\n",
    "    FN = pos - TP\n",
    "    FP = TP*(1.-P)/P\n",
    "    TN = neg-FP\n",
    "    acc = (TP+TN)/(TP+FP+TN+FN)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print '{:.1%}'.format(reverse_engineer_acc(0.73, 0.62))\n",
    "print '{:.1%}'.format(reverse_engineer_acc(0.95, 0.69))\n",
    "print '{:.1%}'.format(reverse_engineer_acc(0.91, 0.71))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the top scores on CMP and GSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%file color_coded_errors.py\n",
    "\n",
    "ERROR_COLORS = [[128, 128,  128], #TN\n",
    "                [255, 0,    0],   #FP\n",
    "                [0,   255,  0],   #TP\n",
    "                [0,   0,    255], #FN\n",
    "                [0,   0,    0], #ignored\n",
    "               ]  \n",
    "ERROR_COLORS = array(ERROR_COLORS, dtype=np.uint8)\n",
    "\n",
    "def color_coded_errors(expected, predicted, ignored=None, colors=ERROR_COLORS):\n",
    "    TP = ~ignored & (expected & predicted)\n",
    "    FN = ~ignored & (expected & ~predicted)\n",
    "    FP = ~ignored & (~expected & predicted)\n",
    "    TN = ~ignored & (~expected & ~predicted)\n",
    "    errors = np.argmax(array([TN, FP, TP, FN, ignored], dtype=np.uint8), axis=0)\n",
    "    if colors is not None:\n",
    "        return np.ma.masked_arra   \n",
    "    #print total\n",
    " y(colors[errors], np.dstack([TN]*3))\n",
    "    else:\n",
    "        return errors  \n",
    "      \n",
    "def render_errors(path, alpha=0.5, noFN=True):\n",
    "    mf, rgb, expected, predicted = get_metrics(path)\n",
    "    ignored = (expected != mf.label_positive) & (expected != mf.label_negative)\n",
    "    expected = expected==mf.label_positive\n",
    "    predicted = predicted==mf.label_positive\n",
    "    print mf.label_positive\n",
    "    cc = color_coded_errors(expected, predicted, ignored)\n",
    "    rgb = rgb.transpose(1,2,0)/255.\n",
    "    if noFN:\n",
    "        rgb[~cc.mask] = (1-alpha)*rgb[~cc.mask] + alpha*cc[~cc.mask]\n",
    "        return rgb.clip(0,1)\n",
    "        #imshow(predicted)\n",
    "    else:\n",
    "        rgb = (1-alpha)*rgb + alpha*cc\n",
    "    return rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(f):\n",
    "    acc = Metrics(**json.loads(eval_results[f])).pixel_f1\n",
    "    if isnan(acc):\n",
    "        acc = 0\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = array([score(f) for f in eval_files])\n",
    "\n",
    "cmp_accs = array([accs[i] for i in range(len(eval_files))  if '/cmp/' in eval_files[i]])\n",
    "cmp_files = array([eval_files[i] for i in range(len(eval_files))  if '/cmp/' in eval_files[i] ])\n",
    "\n",
    "gsv_accs = array([accs[i] for i in range(len(eval_files)) if '/cmp/' not in eval_files[i] ] )\n",
    "gsv_files = array([eval_files[i] for i in range(len(eval_files))  if '/cmp/' not in eval_files[i] ])\n",
    "\n",
    "gsv_ranking = argsort(gsv_accs)\n",
    "cmp_ranking = argsort(cmp_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure()\n",
    "plot(gsv_accs[gsv_ranking])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pdb off"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "gsv_comparison_files = [gsv_files[gsv_ranking[-i-1]] for i in range(9)]\n",
    "\n",
    "with open('gsv_comparison_files.txt', 'w') as f:\n",
    "    f.writelines([fn + '\\n' for fn in comparison_files])\n",
    "\n",
    "# GSV Data\n",
    "\n",
    "fig = figure(figsize=(9,9))\n",
    "plt.subplots_adjust(wspace=0)\n",
    "for i in range(9):\n",
    "    subplot(3,3,i+1)\n",
    "    cached = 'separable-GSV-top-{}.png'.format(i+1)\n",
    "    if os.path.isfile(cached):\n",
    "        err_image = imread(cached)\n",
    "    else:\n",
    "        err_image = render_errors(gsv_files[gsv_ranking[-i-1]], alpha=0.6);\n",
    "        imsave(cached, err_image)\n",
    "    imshow(err_image)\n",
    "    xticks([]); yticks([]); #xlabel(cached, fontsize=8)\n",
    "    fig.canvas.draw()\n",
    "fig.tight_layout()\n",
    "savefig('separable-GSV-top-9-figure.png', dpi=400)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cmp_comparison_files = [cmp_files[cmp_ranking[-i-1]] for i in range(9)]\n",
    "\n",
    "with open('cmp_comparison_files.txt', 'w') as f:\n",
    "    f.writelines([fn + '\\n' for fn in cmp_comparison_files])\n",
    "\n",
    "# CMP Data\n",
    "\n",
    "fig = figure(figsize=(9,9))\n",
    "plt.subplots_adjust(wspace=0)\n",
    "for i in range(9):\n",
    "    subplot(3,3,i+1)\n",
    "    cached = 'separable-CMP-top-{}.png'.format(i+1)\n",
    "    if os.path.isfile(cached):\n",
    "        err_image = imread(cached)\n",
    "    else:\n",
    "        err_image = render_errors(cmp_comparison_files[i], alpha=0.6);\n",
    "        imsave(cached, err_image)\n",
    "    imshow(err_image)\n",
    "    xticks([]); yticks([]); #xlabel(cached, fontsize=8)\n",
    "    fig.canvas.draw()\n",
    "fig.tight_layout()\n",
    "savefig('separable-CMP-top-9-figure.png', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp_comparison_files = [cmp_files[cmp_ranking[-i-1]] for i in range(9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = cmp_comparison_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsv_comparison_files = [gsv_files[gsv_ranking[-i-1]] for i in range(9)]\n",
    "gsv_comparison_files[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(gsv_comparison_files[2])\n",
    "rgb = data[:3]\n",
    "\n",
    "figure()\n",
    "imshow(rgb.transpose(1,2,0)/255)\n",
    "imsave('gsv-example.png', rgb.transpose(1,2,0)/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = net.forward(data=array([rgb]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure()\n",
    "imshow(rgb.transpose(1,2,0)/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = cm.nipy_spectral(linspace(0, 1, 13))[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.morphology import binary_closing, binary_opening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_all(expected=None):\n",
    "    cc = net.blobs['data'].data[0].transpose(1,2,0)/255\n",
    "    labels = np.zeros(cc.shape[:2], dtype=int)\n",
    "    for i, label in enumerate(LABELS):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        if expected is not None:\n",
    "            mask = expected[i] == POS\n",
    "        else:\n",
    "            mask = net.blobs['label-'+label].data[0,0] == POS\n",
    "        #mask = binary_closing(mask, selem=ones((5, 5)))\n",
    "        #mask = binary_opening(mask, selem=ones((5, 5)))\n",
    "        cc[mask] = colors[i][:3]\n",
    "        labels[mask] = i\n",
    "    return cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import regionprops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(9):\n",
    "    data = np.load(gsv_comparison_files[i])\n",
    "    rgb = data[:3]\n",
    "    results = net.forward(data=array([rgb]))\n",
    "\n",
    "    figure(figsize=(8, 3))\n",
    "    subplot(131)\n",
    "    imshow(rgb.transpose(1,2,0)/255)\n",
    "    axis('off')\n",
    "    subplot(132)\n",
    "    imshow(rgb.transpose(1,2,0)/255)\n",
    "    imshow(color_all(), alpha=0.75)\n",
    "    axis('off')\n",
    "    subplot(133)\n",
    "    imshow(rgb.transpose(1,2,0)/255)\n",
    "    imshow(color_all(data[3:]), alpha=0.75)\n",
    "    axis('off')\n",
    "    tight_layout()\n",
    "    subplots_adjust(wspace=0.,hspace=0.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(2, 6))\n",
    "ax = gca()\n",
    "w = 0.2\n",
    "h = 0.05\n",
    "for i, label in enumerate(LABELS):\n",
    "    x = 0\n",
    "    y = i*h\n",
    "    R = Rectangle((x,y), w, h, fill=True, color=colors[i], alpha=1)\n",
    "    ax.add_patch(R)\n",
    "    text(x+w/2, y+h/2, label, horizontalalignment=\"center\", verticalalignment=\"center\")\n",
    "ylim(0, len(LABELS)*h)\n",
    "xlim(0,w)\n",
    "axis('off')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
