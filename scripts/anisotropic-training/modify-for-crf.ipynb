{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modify the separable net for simple crf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This modifies the seprable/low-rank-filter classifier adding inputs from a previous pass of inference. \n",
    "The idea is that you could do this:\n",
    "\n",
    "```python\n",
    "initial_net.forward(data=array([rgb]))\n",
    "probs = [initial_net.blob[prob_feature].data[0,(0,2,3)] for prob_feature in FEATURES]\n",
    "for pass in range(npasses):\n",
    "    results = net.forward(data=array([concatenate([rgb, probs])]))\n",
    "    probs = [net.blob[prob_feature].data[0,(0,2,3)] for prob_feature in FEATURES]\n",
    "```\n",
    "assuming that `net` is a trained net, that `FEATURES` is a list of output probability layer names for each feature.  Then the final result would be similar to CRF optimizatio,\n",
    "\n",
    "\n",
    "Before using this you should:\n",
    "- Have pycaffe (from https://github.com/alexgkendall/caffe-segnet) setup and on the path\n",
    "- Have a copy of solver.prototxt\n",
    "- Have a copy of the modified segnet training classifier (e.g. `modified-training-net.prototxt`)\n",
    "- Have a copy of the 12-target segnet inference classifier (e.g. `modified-inference-net.prototxt`)\n",
    "\n",
    "Table of contents:\n",
    "- [Utility Functions](#Utility-Functions)\n",
    "- [Add Low Rank Filter (Function)](#Add-Low-Rank-Filters)\n",
    "- [Generate the Training Net](#Generate-the-net-to-use-for-training)\n",
    "- [Generate the Inference Net](#Generate-the-net-to-use-for-inference-/-testing)\n",
    "- [Test the Net](#Final-Test)\n",
    "- [Modify the Solver Prototxt](#Set-the-solver-parameters-to-use-the-new-net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google.protobuf.text_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import caffe\n",
    "import caffe.proto.caffe_pb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For debugging (cell can be removed from final notebook)\n",
    "from IPython.core.debugger import Tracer\n",
    "set_trace = Tracer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_net_proto(path):\n",
    "    \"\"\"Read a net from a prototxt file\n",
    "    \n",
    "    :param path: The path to a caffe network (.prototxt file)\n",
    "    \n",
    "    :return: The prototxt object\n",
    "    :rtype: caffe.proto.caffe_pb2.NetParameter\n",
    "    \"\"\"\n",
    "    net = caffe.proto.caffe_pb2.NetParameter()\n",
    "    with open(path) as f:\n",
    "        proto = f.read()\n",
    "    google.protobuf.text_format.Parse(proto, net)\n",
    "    return net\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_net(train_net):\n",
    "    \"\"\" Print a (not sooo short) summary of the net layers\n",
    "    \n",
    "    :param net: A net \n",
    "    :type net: caffe.proto.caffe_pb2.NetParameter\n",
    "    \"\"\"\n",
    "    layers = list(train_net.layer)\n",
    "    for i, layer in enumerate(layers):\n",
    "        print \"{:04}\".format(i),\n",
    "        print \"\\t{:15}\\t{:15}\".format(layer.name, layer.type),\n",
    "        if layer.type==\"Convolution\":\n",
    "            if layer.convolution_param.kernel_size > 0:\n",
    "                print \"\\t{0:>2}x{0:<2}\".format(layer.convolution_param.kernel_size),\n",
    "            else:\n",
    "                print \"\\t{:>2}x{:<2}\".format(layer.convolution_param.kernel_w, layer.convolution_param.kernel_h),                \n",
    "        else:\n",
    "            print \"\\t{:5}\".format(''),\n",
    "\n",
    "        if \"_D\" in layer.name:\n",
    "            print \"DECODE\"\n",
    "        else:\n",
    "            print \"      \"\n",
    "\n",
    "    print \"Total\", len(layers), \"layers\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_net_proto(path, net):\n",
    "    new_proto = google.protobuf.text_format.MessageToString(net)\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(new_proto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modify the inference net to consume output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = ['facade', 'window', 'door', 'cornice', 'sill', 'balcony', 'blind', 'deco', 'molding', 'pillar', 'shop']\n",
    "NUM_FEATURES = len(FEATURES)\n",
    "NEG = NEGATIVE = 0\n",
    "UNK = UNKNOWN = 1\n",
    "POS = POSITIVE = 2\n",
    "EDG = EDGE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inference_net = read_net_proto('modified-inference-net.prototxt')\n",
    "inference_net = read_net_proto('non-bayesian-inference-net.prototxt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print inference_net.input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_net.input_dim[0] = 1 # Batch size of one, since batches are bigger\n",
    "inference_net.input_dim[1] = 3 + NUM_FEATURES*3  # (R,G,B + NUM_FEATURES*(NEG,POS,EDG))\n",
    "print inference_net.input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_net_proto('crf_inference_net.prototxt', inference_net) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only real difference should be in the number of input channels (line 3) that it expects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head crf_inference_net.prototxt -n 5 | cat --number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modify the training net to consume its own output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_net = read_net_proto('modified-training-net.prototxt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_net = train_net\n",
    "\n",
    "new_net= caffe.proto.caffe_pb2.NetParameter()\n",
    "new_net.CopyFrom(orig_net)\n",
    "layers = new_net.layer._values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_layer = layers[0]\n",
    "data_layer.python_param.module = \"python_layers_with_prior\"\n",
    "data_layer.python_param.layer = \"TrainInputLayerWithPrior\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print data_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_net_proto('crf_training_net.prototxt', new_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make the new python layer for training..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I split the python input layer into two files:\n",
    "- [prepare_crf_input.py](/edit/scripts/anisotropic-training/prepare_crf_input.py) -- Uses the inference net to get labels using the non-crf network.\n",
    "- [python_layers_with_prior.py](/edit/scripts/anisotropic-training/python_layers_with_prior.py) -- Feeds a network with the best non-crf estimate of the labels as additional inputs. The idea is that we could use a net trained this way in a loop..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer the weights in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import caffe\n",
    "caffe.set_mode_cpu()  # No need to consume CPU for this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "with warnings.catch_warnings('ignore'):\n",
    "    old_net = caffe.Net('modified-training-net.prototxt', 'deploy/test_weights.caffemodel', caffe.TEST)\n",
    "    new_net = caffe.Net('crf_training_net.prototxt', caffe.TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import truncnorm\n",
    "truncated_normal = truncnorm(-1, 1).rvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in old_net.params:\n",
    "    if param == 'conv1_1':\n",
    "        N,C,H,W = new_net.params['conv1_1'][0].data.shape\n",
    "        new_net.params['conv1_1'][0].data[:, :3, :, :] = old_net.params['conv1_1'][0].data[...]\n",
    "        new_net.params['conv1_1'][0].data[:, 3:, :, :] = truncated_normal(size=(N, C-3, H, W))\n",
    "        new_net.params['conv1_1'][1].data[...] = old_net.params['conv1_1'][1].data[...]\n",
    "    else:\n",
    "        for i in range(len(old_net.params[param])):\n",
    "            new_net.params[param][i].data[...] = old_net.params[param][i].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_net.save('crf_initial_weights.caffemodel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the solver parameters to use the new net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import caffe\n",
    "import os\n",
    "import google.protobuf.text_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = caffe.proto.caffe_pb2.SolverParameter()\n",
    "google.protobuf.text_format.Merge(open('solver.prototxt').read(), solver);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver.net = os.path.abspath('crf_training_net.prototxt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver.snapshot_prefix = os.path.join(os.path.dirname(solver.snapshot_prefix), \"crf_facades\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {os.path.dirname(solver.snapshot_prefix)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('crf_solver.prototxt', 'w') as f:\n",
    "    f.write(google.protobuf.text_format.MessageToString(solver))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make sure the GPU is ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_net = None; del old_net\n",
    "new_net = None; del new_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_I_ am not using the GPU, but there may be some zombie processes claiming some of its RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print os.getpid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kill 27026\n",
    "# !kill 6354\n",
    "# !kill 17722"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this training loop, I need to save the outputs after each iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import caffe\n",
    "import os\n",
    "caffe.set_mode_gpu()\n",
    "caffe.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anydbm\n",
    "import json\n",
    "\n",
    "history = anydbm.open('crf_training_history', 'c')\n",
    "\n",
    "iteration = len(history)\n",
    "print iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings('ignore'):\n",
    "    solver = caffe.SGDSolver('crf_solver.prototxt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if iteration == 0:\n",
    "    solver.net.copy_from('crf_initial_weights.caffemodel')\n",
    "else:\n",
    "    iteration = iteration - iteration % 1000\n",
    "    snapshot = '/home/shared/Projects/Facades/mybook/anisotropic/crf_facades_iter_{}.solverstate'\n",
    "    snapshot = snapshot.format(iteration)\n",
    "    solver.restore(snapshot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver.net.layers[0].verbose = False\n",
    "solver.net.epochs = iteration % len(solver.net.layers[0].files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def softmax(a, axis=0):\n",
    "    a = np.exp(a - a.max(axis=axis))\n",
    "    a /= a.sum(axis=axis)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyfacades.util import softmax\n",
    "#  ^--- my softmax was not normalizing properly...\n",
    "\n",
    "\n",
    "def visualize_progress(fig, prior=None, prior_loss=0):\n",
    "    #imshow(solver.net.blobs['concat'].data[0, 6])\n",
    "    fig.clf()\n",
    "    subplot(221)\n",
    "    imshow(solver.net.blobs['data'].data[0,:3].transpose(1,2,0)/255.)\n",
    "    xticks([]);yticks([]);xlabel('rgb')\n",
    "    subplot(222)\n",
    "    imshow(solver.net.blobs['window'].data[0,0])  # Color(3) + Windows(1)*NumLabels(3) + Positive(1)\n",
    "    xticks([]);yticks([]);xlabel('expected')\n",
    "    subplot(223)\n",
    "    if prior is not None:\n",
    "        imshow(prior)  # Color(3) + Windows(1)*NumLabels(3) + Positive(1)\n",
    "        xticks([]);yticks([]);xlabel('intitial({:.3f})'.format(prior_loss))\n",
    "    subplot(224)\n",
    "    imshow(softmax(solver.net.blobs['conv-window'].data[0])[2])\n",
    "    current_loss = float(solver.net.blobs['window-loss'].data)\n",
    "    xticks([]);yticks([]);xlabel('current({:.3f})'.format(current_loss))\n",
    "    #fig.canvas.draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from munch import Munch\n",
    "def log_image():\n",
    "    record = Munch()\n",
    "    input_layer  = solver.net.layers[0]\n",
    "    record.time = datetime.datetime.isoformat(datetime.datetime.now())\n",
    "    record.epoch = input_layer.epochs\n",
    "    record.image = input_layer.files[input_layer.counter]\n",
    "    record.loss = {loss_layer.replace('-loss',''):float(solver.net.blobs[loss_layer].data) for loss_layer in solver.net.outputs}\n",
    "    record.total_loss = sum(record.loss.values())\n",
    "    record.iteration = iteration\n",
    "    \n",
    "    history[json.dumps(iteration)] = json.dumps(record)\n",
    "    \n",
    "    return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from munch import munchify\n",
    "def get_log_record(i):\n",
    "    return munchify(json.loads(history[json.dumps(i)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print get_log_record(100).loss.window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_losses(n=1000):\n",
    "    n = min(n, iteration)\n",
    "    iterations = arange(iteration-n, iteration)\n",
    "    losses = [get_log_record(i).loss.window for i in iterations]       \n",
    "    return iterations, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import gaussian, convolve\n",
    "\n",
    "FILTER_HWIDTH=50\n",
    "FILTER =  gaussian(2*FILTER_HWIDTH+1, 25)\n",
    "FILTER /= sum(FILTER)\n",
    "\n",
    "def plot_last_losses(n=1000, ax = None):\n",
    "    ax = ax or gca()\n",
    "    i, loss = get_last_losses(n)\n",
    "    ax.plot(i, loss, alpha=0.2, c='red')\n",
    "    ax.plot(i[FILTER_HWIDTH:-FILTER_HWIDTH],  convolve(loss, FILTER, mode='valid'))\n",
    "    ax.set_xlim(i[0], i[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "The cell below executes the training loop. This can take a LOONG time, so I do not want to execute it by accident as I work through the notebook. \n",
    "\n",
    "I have disabled the cell by marking it as a `raw` cell in jupyter. In order to train the net you need to convert the next cell to `python` again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"We are currently at iteration\", iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = ['facade', 'window', 'door', 'cornice', 'sill', 'balcony', 'blind', 'deco', 'molding', 'pillar', 'shop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import python_layers_with_prior\n",
    "reload(python_layers_with_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver.net.layers[0].epochs = iteration / len(solver.net.layers[0].files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pdb on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = figure(figsize=(12,5))\n",
    "\n",
    "for i in range(1000000):\n",
    "    solver.net.layers[0].priors = None\n",
    "    for j in range(10):\n",
    "        solver.step(1)\n",
    "        if j == 0:\n",
    "            prior = solver.net.blobs['data'].data[0, 2*3+1].copy()\n",
    "            prior_loss = float(solver.net.blobs['window-loss'].data)\n",
    "        probs = [softmax(solver.net.blobs['conv-{}'.format(feature)].data[0,(0,2,3)]) for feature in FEATURES]\n",
    "        probs = np.concatenate(probs, axis=0)\n",
    "        solver.net.layers[0].priors = probs\n",
    "        visualize_progress(fig, prior, prior_loss)\n",
    "        suptitle('step {}, epoch {}, image {}, iter {}'.format(i, solver.net.layers[0].epochs,solver.net.layers[0].counter,  j))\n",
    "        fig.canvas.draw()\n",
    "    log_image()\n",
    "    iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(frameon=False)\n",
    "plot_last_losses(1e10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Compute the batch normalization\n",
    "This competes with training for access to the GPU -- best restart the Kernel first\n",
    "\n",
    "> **ATTENTION:** _You need to restart the kernel here before proceeding!_\n",
    "\n",
    "The GPU does not have enough RAM to hold the models for both BN and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab notebook\n",
    "import anydbm\n",
    "import json\n",
    "\n",
    "history = anydbm.open('crf_training_history', 'c')\n",
    "iteration = len(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash --out model_file \n",
    "./get_iter.py -m -p crf_solver.prototxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = model_file.strip()\n",
    "print model_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **WARNING:** The next cell takes about 90 minutes to complete\n",
    "So I have commented it out -- you will need to uncomment it to run it again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p crf-deploy\n",
    "%run compute_bn_statistics.py crf_training_net.prototxt {model_file.strip()}  crf-deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For whatever reason, the notebook is a bit.. kludgy after the BN procedure finishes. I suspect that I am leaking some resource when I do all of the plotting, or perhaps I produce to much output. \n",
    "\n",
    "You may need to comment out line 2 in the cell above and re-run the cell up to this point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitatively Evaluate the net (after applying BN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should be able to load a crf inference net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE:** After running BN it is a good idea to restart the kernel, etc. and make sure that the GPU RAM is not being hogged..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab notebook\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CPU = False\n",
    "\n",
    "import caffe\n",
    "if CPU:\n",
    "    caffe.set_mode_cpu()\n",
    "else:\n",
    "    caffe.set_mode_gpu()\n",
    "    caffe.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = caffe.Net('crf_inference_net.prototxt', 'crf-deploy/test_weights.caffemodel', caffe.TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_FILES = [fn.strip() for fn in open('./data/training/independant_12_layers/fold_01/eval.txt')]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from prepare_crf_input import prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load prepare_crf_input.py\n",
    "\n",
    "FEATURES = ['facade', 'window', 'door', 'cornice', 'sill', 'balcony', 'blind', 'deco', 'molding', 'pillar', 'shop']\n",
    "WEIGHT = 'deploy/test_weights.caffemodel'\n",
    "LAYOUT = 'modified-inference-net.prototxt'\n",
    "\n",
    "NEG = NEGATIVE = 0\n",
    "POS = POSITIVE = 2\n",
    "EDG = EDGE = 3\n",
    "\n",
    "import numpy as np\n",
    "import caffe\n",
    "init_net = caffe.Net(LAYOUT, WEIGHT, caffe.TEST)\n",
    "\n",
    "# Set the batch size to one\n",
    "init_net.blobs['data'].reshape(1, 3, 512, 512)\n",
    "init_net.reshape()\n",
    "\n",
    "def priors(im):\n",
    "    \"\"\"\n",
    "    :param im: An input image, shape 3x512x512\n",
    "    :type im: np.ndarray\n",
    "    \"\"\"\n",
    "    result = init_net.forward(data=np.array([im[:3]]))\n",
    "    probs = [init_net.blobs['prob-{}'.format(feature)].data[0,(NEG,POS,EDG)] for feature in FEATURES]\n",
    "    probs = np.concatenate(probs, axis=0)\n",
    "    return probs\n",
    "\n",
    "def iterate(net):\n",
    "    probs = [net.blobs['prob-{}'.format(feature)].data[0,(NEG,POS,EDG)] for feature in FEATURES]\n",
    "    probs = np.concatenate(probs, axis=0)\n",
    "    net.blobs['data'].data[0,3:] = probs\n",
    "    result = net.forward(data=net.blobs['data'].data)\n",
    "    return result\n",
    "    \n",
    "def prepare(im, prior=None):\n",
    "    if prior is None:\n",
    "        prior = priors(im)\n",
    "    assert prior.shape == (len(FEATURES)*3, 512, 512)\n",
    "    concat = np.concatenate([im[:3], prior])\n",
    "    return np.array([concat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_iter(fig=None):\n",
    "    fig = fig or figure(figsize=(10,4))\n",
    "    subplot(131)\n",
    "    imshow(net.blobs['data'].data[0,:3].transpose(1,2,0)/255.)\n",
    "    axis('off')\n",
    "    subplot(132)\n",
    "    imshow(net.blobs['data'].data[0,:3].transpose(1,2,0)/255.)\n",
    "    imshow(net.blobs['prob-window'].data[0,2], alpha=0.65)\n",
    "    axis('off')\n",
    "    subplot(133)\n",
    "    imshow(net.blobs['prob-window'].data[0,2])\n",
    "    axis('off')\n",
    "    tight_layout()\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "figure(figsize=(8, 4))\n",
    "subplot(121)\n",
    "imshow(data.transpose(1,2,0)/255.)\n",
    "xticks([]); yticks([])\n",
    "subplot(122)\n",
    "imshow(targets[1+FEATURES.index('window')])\n",
    "xticks([]); yticks([])\n",
    "tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 24\n",
    "data_with_targets = np.load(EVAL_FILES[idx])\n",
    "data, targets = data_with_targets[:3], data_with_targets[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = 'frame-image-{}'.format(idx)\n",
    "!mkdir -p {dirname}\n",
    "\n",
    "results = net.forward(data=prepare(data))\n",
    "fig = figure(figsize=(10,4))\n",
    "for i in range(1000):\n",
    "    iterate(net)\n",
    "    title('Iteratin {}'.format(i))\n",
    "    plot_iter(fig)\n",
    "    savefig(os.path.join(dirname, 'frame_{:05}'.format(i+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ffmpeg -i frame-image-{idx}/frame_%05d.png -vcodec libx264 -crf 25  -pix_fmt yuv420p  video-image-{idx}.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = iterate(net)\n",
    "plot_iter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = iterate(net)\n",
    "plot_iter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = iterate(net)\n",
    "plot_iter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = iterate(net)\n",
    "plot_iter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = iterate(net)\n",
    "plot_iter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = iterate(net)\n",
    "plot_iter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = iterate(net)\n",
    "plot_iter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = iterate(net)\n",
    "plot_iter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = iterate(net)\n",
    "plot_iter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = iterate(net)\n",
    "plot_iter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = iterate(net)\n",
    "plot_iter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = iterate(net)\n",
    "plot_iter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = iterate(net)\n",
    "plot_iter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = iterate(net)\n",
    "plot_iter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
